<html>
<body>
<h1>Big data main page:</h1>

<h2>links:</h2>
<li><a href="index.html">Big data main page</a></li>
<li><a href="Definitionofbigdata.html">Definition of Big data </a></li>
<li><a href="Historyofbigdata.html">History of big data</a></li>
<li><a href="typesofbigdata.html">types of big data</a></li>
<li><a href="Applicationofbigdata.html">Application of big data</a></li>

<h2>introduction</h2>

<img src="WhatsApp Image 2020-06-01 at 1.30.28 PM.jpeg" alt="Big Data"><br>

Big data is a blanket term for the non-traditional strategies and technologies needed to gather, organize, process, and gather insights from large datasets.
 While the problem of working with data that exceeds the computing power or storage of a single computer is not new, the pervasiveness, scale, and value of 
 this type of computing has greatly expanded in recent years.In this article, we will talk about big data on a fundamental level and define common concepts
 you might come across while researching the subject. We will also take a high-level look at some of the processes and technologies currently being used in
 this space.
The basic requirements for working with big data are the same as the requirements for working with datasets of any size. However, the massive scale,
 the speed of ingesting and processing, and the characteristics of the data that must be dealt with at each stage of the process present significant
 new challenges when designing solutions. The goal of most big data systems is to surface insights and connections from large volumes of heterogeneous
 data that would not be possible using conventional methods.
In 2001, Gartner’s Doug Laney first presented what became known as the “three Vs of big data” to describe some of the characteristics that make big data
 different from other data processing:
Volume
The sheer scale of the information processed helps define big data systems. These datasets can be orders of magnitude larger than traditional datasets, which demands more thought at each stage of the processing and storage life cycle.
Often, because the work requirements exceed the capabilities of a single computer, this becomes a challenge of pooling, allocating, and coordinating 
resources from groups of computers. Cluster management and algorithms capable of breaking tasks into smaller pieces become increasingly important.

</body>
</html>
